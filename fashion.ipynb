{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A \n",
    "## Name : Joel Poah\n",
    "## Class: DAAA/2B/06\n",
    "## Admin No: 2112729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning classifcation on Fashion MNIST \n",
    "\n",
    "## Overview \n",
    "### Fashion MNIST is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST.\n",
    "| Label       | Description |\n",
    "| ----------- | ----------- |\n",
    "| 0           | T-shirt/Top |\n",
    "| 1           | Trouser     |\n",
    "| 2           | Pullover    |\n",
    "| 3           | Dress       |\n",
    "| 4           | Coat        |\n",
    "| 5           | Sandal      |\n",
    "| 6           | Shirt       |\n",
    "| 7           | Sneaker     |\n",
    "| 8           | Bag         |\n",
    "| 9           | Ankle Boot  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "data = tf.keras.datasets.fashion_mnist.load_data()\n",
    "# (X_train, y_train), (X_test, y_test) = data\n",
    "\n",
    "# split data into train , test and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[0][0], data[0][1], test_size=0.2, random_state=42) \n",
    "# 80% train, 20% test\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42) # 0.2 x 0.8 = 0.16 = 16% of original data\n",
    "# we have 64% train,20% test,16% validation\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train.reshape(-1, 784))\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "X_train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pixel normalization\n",
    "> Dividing each of the pixels by 255 will normalize the pixels between 0 to 1 \n",
    "We normalize the pixels so that it can increase the speed of the learning process\n",
    "Neural Network processes inputs uses small weights values. Large inputs can disrupt or slow down learning process.\n",
    "It is good that we normalize the pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.min(), X_train.max())\n",
    "# it seems like the data set provided by tensorflow is already between 0 to 255 . We can normalize it by dividing by 255 or so called pixel normalization\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# X_val = X_val / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = y_train_df[0].unique()\n",
    "types.sort()\n",
    "print(types)\n",
    "print(y_train_df[0].value_counts())\n",
    "# y_ train is just an array of numbers from 0 to 9 that represent the class of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_each_class_position = [1,21,5,20,24,9,18,6,23,0]\n",
    "# plot all images in array_of_each_class_position\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.title(class_names[i])\n",
    "    plt.imshow(data[0][0][array_of_each_class_position[i]], cmap=plt.cm.binary)\n",
    "    plt.xlabel(data[0][1][array_of_each_class_position[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train: uint8 NumPy array of grayscale image data with shapes (, 28, 28), containing the training data.\n",
    "\n",
    "- y_train: uint8 NumPy array of labels (integers in range 0-9) with shape (60000,) for the training data.\n",
    "\n",
    "- X_test: uint8 NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data.\n",
    "\n",
    "- y_test: uint8 NumPy array of labels (integers in range 0-9) with shape (10000,) for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Neural Network / Multi layer perceptrons (MLPs) \n",
    "## MLP is a Artificial Neural Network consisting of at least 3 layers ( input , hidden layer and output) \n",
    "\n",
    "![](./images/DenseNN.gif) \n",
    "\n",
    "## **Trying out Dense neural networks** \n",
    "### 1 Layered Densely connected neural network \n",
    "- It is 1 dimensional \n",
    "- image was 28 by 28 pixels but flattened to an array of attributes (784) \n",
    "- Neural network takes in 784 columns of attributes as inputs and pass to 128 neurons in the example shown below\n",
    "- Hidden layer then passes to the sigmoid function which generates a prediction percentage for the 10 classes\n",
    "\n",
    "#### Things i discovered while experimenting with 1 layered dense neural network\n",
    "- loss='sparse_categorical_crossentropy\" is used for label encoded data\n",
    "- input shape of the Dense layer have to match the amount of attributes in dataset or else error thrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline simple neural network\n",
    "from tensorflow.keras.layers import Flatten\n",
    "# model = keras.Sequential(\n",
    "#     [\n",
    "#         keras.Input(shape=(28, 28)),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(128, activation=\"relu\"), # 128 neurons\n",
    "#         layers.Dense(10,activation='sigmoid')                  # 10 neurons as there are 10 types of fashion clothing\n",
    "\n",
    "#     ]\n",
    "# )\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(128, input_shape=(784,), activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=5, verbose=2)\n",
    "results=model.evaluate(X_test, y_test,verbose=2)\n",
    "print(\"test loss,\", results[0], \"test acc:\", results[1])\n",
    "\n",
    "# Train your model and save its history\n",
    "def plot_loss(loss,test_loss):\n",
    "  plt.figure()\n",
    "  plt.plot(loss)\n",
    "  plt.plot(test_loss)\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "def plot_accuracy(acc,test_acc):\n",
    "    plt.figure()\n",
    "    plt.plot(acc)\n",
    "    plt.plot(test_acc)\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "  \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "h_callback = model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layer Dense Neural network\n",
    "> Baseline model with at least 2 layers which performs slightly better than 1 layer\n",
    "However 2 layered model is clearly overfitted where testing learning curve diverges away from training learning curve \n",
    "and only performs a little better than model with 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline simple neural network\n",
    "# model = keras.Sequential(\n",
    "#     [\n",
    "#         keras.Input(shape=(28, 28)),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(128, activation=\"relu\"), # 128 neurons\n",
    "#         layers.Dense(10,activation='sigmoid')                  # 10 neurons as there are 10 types of fashion clothing\n",
    "\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "#flatten X_train and X_test\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(784,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "h_callback = model.fit(X_train, y_train,epochs = 5,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "results=model.evaluate(X_test, y_test, batch_size=32, verbose=2)\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 layered MLP \n",
    "#### Backward propagation behind the scenes of a MLP Artificial Neural Network\n",
    "![](./images/Backpropagation.gif)  ![](./images/GradientDescent.gif) \n",
    "- Understood why with every epoch accuracy is increased\n",
    "- BackwardPropagation adjusts weights using calculas(gradient descent) finding global minimum error of the function\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(784,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "h_callback = model.fit(X_train, y_train,epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "results=model.evaluate(X_test, y_test, batch_size=32, verbose=2)\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.reshape(X_val.shape[0], 784,1)\n",
    "pred=model.predict(X_val)\n",
    "pred=np.argmax(pred,axis=1)\n",
    "classifation_matrix=confusion_matrix(y_val,pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(classifation_matrix,annot=True,fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "pros\n",
    "- simple model\n",
    "- easier to build \n",
    "\n",
    "cons\n",
    "- can't alter input size after model is trained\n",
    "- permutation invariance , everything is fully connected as they are dense layers\n",
    "- Cannot perform well on those images that have the same shape\n",
    "\n",
    "### As shown on classification matrix , this dense neural network don't perform well on classes 9(Ankle Boots) and 7(Sneaker) and 5(Sandal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "\n",
    "![](./images/IntroCNN.jpeg)\n",
    "\n",
    "## Documentation of how Convolving or Applied Filtering works in CNN\n",
    "Saily Shah — Published On January 27, 2022 and Last Modified On March 15th, 2022\n",
    "Shah,S *Convolutional Neural Network: An Overview* Available at: https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c[Accessed : 4 november 2022 ]\n",
    "> Convolve ~ combine (one function or series) with another by forming their convolution which is also summing the element-wise product of 2 matrices (Shah,S 2022)\n",
    "\n",
    "![](./images/convolved_z.png)\n",
    "- Dot product performed between input and filter( filter is a measure of how close a region of the input resembles a feature; it can be a vertical line,horizontal edge or diagonal)\n",
    "- values found in filter(2nd matrix, *filter is also known as the kernel_size in keras*) are the weights that will be learned and tuned with backpropagation( finding global minimum error of function) in the training of the network \n",
    "- sum of the product of the matrix is 70 here\n",
    "> Striding convolutions : repeating what happen above but with strides; the filter moving through the image and getting a matrix of convolved features. The main purpose of doing this is to find features of a class which will then be used for classification of the images\n",
    "\n",
    "> ![](./images/convolve.gif)\n",
    "- final matrix(convolved matrix) produced is smaller after retrieving main features of an image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Documentation of how Pooling works in CNN\n",
    "![](./images/Pooling.gif)\n",
    "- Reducing spatial size this helps *reduce computational power to process data* (also known as dimensionality reduction)\n",
    "- Helps in retrieving dominant features\n",
    "### Max Pooling\n",
    "- With every stride the highest number found is retrieved in an area\n",
    "- Effective as a noise suppresant discarding all noisy activations and only taking the prominant feature in an area \n",
    "\n",
    "### Average Pooling \n",
    "- Retrieves the average of all the values in an area\n",
    "- may not perform as well as max pooling since it is just averaging the noise in images\n",
    "\n",
    "\n",
    "## Why do we need Flattening\n",
    "- Pooled feature maps are 2 Dimensional arrays \n",
    "- Usually Dense layers are used at the last part of CNNs which requires flatten or else there will be an error\n",
    "- convert data into a 1 dimensional array in order to pass to the fully connected layer using Flatten\n",
    "- Flatten is required unless last layer uses global pooling\n",
    "\n",
    "## General conclusion on Convolving and Pooling\n",
    "> Convolving helps neural network extract features performing better than MLP. My hypothesis for this problem is that I think Max pooling may perform better but will try it out . \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "def create_trace(x,y,ylabel,color):\n",
    "        trace = go.Scatter(\n",
    "            x = x,y = y,\n",
    "            name=ylabel,\n",
    "            marker=dict(color=color),\n",
    "            mode = \"markers+lines\",\n",
    "            text=x\n",
    "        )\n",
    "        return trace\n",
    "    \n",
    "def plot_accuracy_and_loss(train_model):\n",
    "    hist = train_model.history\n",
    "    acc = hist['accuracy']\n",
    "    val_acc = hist['val_accuracy']\n",
    "    loss = hist['loss']\n",
    "    val_loss = hist['val_loss']\n",
    "    epochs = list(range(1,len(acc)+1))\n",
    "    \n",
    "    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n",
    "    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n",
    "    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n",
    "    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n",
    "   \n",
    "    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n",
    "                                                             'Training and validation loss'))\n",
    "    fig.append_trace(trace_ta,1,1)\n",
    "    fig.append_trace(trace_va,1,1)\n",
    "    fig.append_trace(trace_tl,1,2)\n",
    "    fig.append_trace(trace_vl,1,2)\n",
    "    fig['layout']['xaxis'].update(title = 'Epoch')\n",
    "    fig['layout']['xaxis2'].update(title = 'Epoch')\n",
    "    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n",
    "    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n",
    "\n",
    "    \n",
    "    iplot(fig, filename=f'accuracy-loss_{train_model}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (stupid baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU\n",
    "epochs = 15\n",
    "num_classes = 10\n",
    "seed = np.random.seed(1)\n",
    "\n",
    "print('before',X_train.shape,X_test.shape)\n",
    "print('before',y_train.shape,y_test.shape)\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n",
    "# print('after',X_train.shape,X_test.shape)\n",
    "# print('after',y_train.shape,y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1)))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "fashion_model.summary()\n",
    "\n",
    "h_callback = fashion_model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training \n",
    "plot_accuracy_and_loss(h_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added batch sizing to see if it affects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model_batch = Sequential()\n",
    "fashion_model_batch.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1)))\n",
    "fashion_model_batch.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "fashion_model_batch.add(Flatten())\n",
    "fashion_model_batch.add(Dense(128, activation='linear'))\n",
    "fashion_model_batch.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model_batch.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "fashion_model_batch.summary()\n",
    "\n",
    "h_callback = fashion_model.fit(X_train, y_train, epochs = 10, batch_size=64,validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training \n",
    "plot_accuracy_and_loss(h_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch sizing does improve performance slightly \n",
    ">The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. Instead of having to update only after the whole dataset is trained once. It is indeed better to update after training each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear activation does not seem to perform well \n",
    "\n",
    "## Trying out different activations \n",
    "![](./images/relu.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying out relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU\n",
    "epochs = 15\n",
    "num_classes = 10\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1)))\n",
    "fashion_model.add(MaxPooling2D((2, 2)))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='relu'))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "fashion_model.summary()\n",
    "\n",
    "h_callback = fashion_model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training \n",
    "plot_accuracy_and_loss(h_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU\n",
    "epochs = 15\n",
    "num_classes = 10\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='tanh',input_shape=(28,28,1)))\n",
    "fashion_model.add(MaxPooling2D((2, 2)))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='tanh'))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "fashion_model.summary()\n",
    "\n",
    "h_callback = fashion_model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training \n",
    "plot_accuracy_and_loss(h_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Out LeakyRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU\n",
    "# fix random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# define cnn model\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "fashion_model.summary()\n",
    "\n",
    "h_callback = fashion_model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU\n",
    "epochs = 15\n",
    "num_classes = 10\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='sigmoid',input_shape=(28,28,1)))\n",
    "fashion_model.add(MaxPooling2D((2, 2)))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='sigmoid'))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='sigmoid'))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "fashion_model.summary()\n",
    "\n",
    "h_callback = fashion_model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training \n",
    "plot_accuracy_and_loss(h_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand the filters in the model \n",
    "#Let us pick the first hidden layer as the layer of interest.\n",
    "layer = fashion_model.layers #Conv layers at 0, \n",
    "filters, biases = fashion_model.layers[0].get_weights()\n",
    "print(layer[0].name, filters.shape)\n",
    "\n",
    "# plot filters\n",
    "\n",
    "fig1=plt.figure(figsize=(8, 12))\n",
    "columns = 8\n",
    "rows = 8\n",
    "n_filters = 32 ## the number of filters in our first layer \n",
    "for i in range(1, n_filters + 1):\n",
    "    f = filters[:, :, :, i-1]\n",
    "    fig1 =plt.subplot(rows, columns, i)\n",
    "    fig1.set_xticks([])  #Turn off axis\n",
    "    fig1.set_yticks([])\n",
    "    plt.imshow(f[:, :, 0], cmap='gray') #Show only the filters from 0th channel (R)\n",
    "    #ix += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "# reshape dataset to have a single channel\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3\n",
    "model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "# Flatten the previous layer output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "h_callback = model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1),padding='same'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Conv2D(32, kernel_size=3, activation='relu',padding='same'))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Conv2D(24, kernel_size=3, activation='relu',padding='same'))\n",
    "model_2.add(Dropout(0.4))\n",
    "model_2.add(Conv2D(64, kernel_size=3, activation='relu',padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_2.add(Dropout(0.3))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(128, activation='relu'))\n",
    "model_2.add(Dropout(0.3))\n",
    "model_2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='adam', metrics=['accuracy'], loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "h_callback = model_2.fit(X_train, y_train, epochs = 10,\n",
    "                validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out different optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "> Instead of computing the gradients over the entire dataset, it performs a parameter update for each example in the dataset.The problem of SGD is that the updates are frequent and with a high variance, so the objective function heavily fluctuates during training.This fluctuation can be an advantage with respect to batch gradient descent because it allows the function to jump to better local minima, but at the same time it can represent a disadvantage with respect to the convergence in a specific local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    ">It adapts the learning rate to the parameters performing small updates for frequently occurring features and large updates for the rarest ones.In this way, the network is able to capture information belonging to features that are not frequent, putting them in evidence and giving them the right weight.The problem of Adagrad is that it adjusts the learning rate for each parameter according to all the past gradients. So, the possibility of having a very small learning rate after a high number of steps — resulting from the accumulation of all the past gradients — is relevant.If the learning rate is too much small, we simply can’t update weights and the consequence is that the network doesn’t learn anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "> It improves the previous algorithm by introducing a history window which sets a fixed number of past gradients to take in consideration during the training.In this way, we don’t have the problem of the vanishing learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    ">It is very similar to Adadelta. The only difference is in the way they manage the past gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    ">It adds to the advantages of Adadelta and RMSprop, the storing of an exponentially decaying average of past gradients similar to momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_2(hp):\n",
    "    optimizers = ['adam', 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adamax', 'nadam']\n",
    "    # activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                    activation='relu',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    #loop through the optimizers\n",
    "    for optimizer in optimizers:\n",
    "        model.compile(optimizer=optimizer(hp.Choice('learning_rate',values=[1e-2,1e-4]),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy']))\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax', kernel_regularizer=tensorflow.keras.regularizers.L2(l2=0.05)))\n",
    "#loop through the optimizers\n",
    "model.compile(optimizer='adam',\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "metrics=['accuracy'])  \n",
    "model.summary()\n",
    "\n",
    "h_callback = model.fit(X_train, y_train, epochs = 10,\n",
    "                validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv 2d by machine learning mastery\n",
    "![](./images/convolutionalNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "# reshape dataset to have a single channel\n",
    "print(trainX.shape, testX.shape)\n",
    "print(trainY.shape, testY.shape)\n",
    "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "trainX=trainX/255\n",
    "testX=testX/255\n",
    "# one hot encode target values\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)\n",
    "print(trainX.shape, testX.shape)\n",
    "print(trainY.shape, testY.shape)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(trainX,trainY, verbose=0)\n",
    "# evaluate model\n",
    "\n",
    "h_callback = model.fit(trainX, trainY, epochs = 10,\n",
    "               validation_data=(testX, testY))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline cnn model for fashion mnist\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "\t# reshape dataset to have a single channel\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(dataX, dataY, n_folds=5):\n",
    "\tscores, histories = list(), list()\n",
    "\t# prepare cross validation\n",
    "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(dataX):\n",
    "\t\t# define model\n",
    "\t\tmodel = define_model()\n",
    "\t\t# select rows for train and test\n",
    "\t\ttrainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\t\t# fit model\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "\t\t# evaluate model\n",
    "\t\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\t\tprint('> %.3f' % (acc * 100.0))\n",
    "\t\t# append scores\n",
    "\t\tscores.append(acc)\n",
    "\t\thistories.append(history)\n",
    "\treturn scores, histories\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t# plot loss\n",
    "\t\tpyplot.subplot(211)\n",
    "\t\tpyplot.title('Cross Entropy Loss')\n",
    "\t\tpyplot.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "\t\tpyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "\t\t# plot accuracy\n",
    "\t\tpyplot.subplot(212)\n",
    "\t\tpyplot.title('Classification Accuracy')\n",
    "\t\tpyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "\t\tpyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\t\tpyplot.legend()\n",
    "\tpyplot.show()\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "\t# print summary\n",
    "\tprint('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n",
    "\t# box and whisker plots of results\n",
    "\tpyplot.boxplot(scores)\n",
    "\tpyplot.show()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\ttrainX, testX = prep_pixels(trainX, testX)\n",
    "\t# evaluate model\n",
    "\tscores, histories = evaluate_model(trainX, trainY)\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(histories)\n",
    "\t# summarize estimated performance\n",
    "\tsummarize_performance(scores)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does it perform on validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.reshape((X_val.shape[0], 28, 28, 1))\n",
    "y_val = to_categorical(y_val)\n",
    "pred = model.predict(X_val)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "classifation_matrix = confusion_matrix(y_val, pred)\n",
    "# plot confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(classifation_matrix, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results was a lot better as compared to the previous multilayer perceptron neural network\n",
    "- There wasn't as much confusion between class 5 , 7 and 8 \n",
    "### However we can still improve the model in predicting classes 2(pullover) , 4(Coat) and 6(Shirt) . Model also thinks that class 6(shirt) may be a class 0(T-shirt) occasionally it looks like the model may be confused because of the shape of the image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving on Machine Learning Mastery model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline cnn model for fashion mnist\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "\t# reshape dataset to have a single channel\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(dataX, dataY, n_folds=5):\n",
    "\tscores, histories = list(), list()\n",
    "\t# prepare cross validation\n",
    "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(dataX):\n",
    "\t\t# define model\n",
    "\t\tmodel = define_model()\n",
    "\t\t# select rows for train and test\n",
    "\t\ttrainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\t\t# fit model\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "\t\t# evaluate model\n",
    "\t\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\t\tprint('> %.3f' % (acc * 100.0))\n",
    "\t\t# append scores\n",
    "\t\tscores.append(acc)\n",
    "\t\thistories.append(history)\n",
    "\treturn scores, histories\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t# plot loss\n",
    "\t\tpyplot.subplot(211)\n",
    "\t\tpyplot.title('Cross Entropy Loss')\n",
    "\t\tpyplot.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "\t\tpyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "\t\t# plot accuracy\n",
    "\t\tpyplot.subplot(212)\n",
    "\t\tpyplot.title('Classification Accuracy')\n",
    "\t\tpyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "\t\tpyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\t\tpyplot.legend()\n",
    "\tpyplot.show()\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "\t# print summary\n",
    "\tprint('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n",
    "\t# box and whisker plots of results\n",
    "\tpyplot.boxplot(scores)\n",
    "\tpyplot.show()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\ttrainX, testX = prep_pixels(trainX, testX)\n",
    "\t# evaluate model\n",
    "\tscores, histories = evaluate_model(trainX, trainY)\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(histories)\n",
    "\t# summarize estimated performance\n",
    "\tsummarize_performance(scores)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()\n",
    "\n",
    "\n",
    "X_val = X_val.reshape((X_val.shape[0], 28, 28, 1))\n",
    "y_val = to_categorical(y_val)\n",
    "pred = model.predict(X_val)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "classifation_matrix = confusion_matrix(y_val, pred)\n",
    "# plot confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(classifation_matrix, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting data augmentation to improve machine learning mastery code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  \n",
    "    samplewise_center=False, \n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,  \n",
    "    zca_whitening=False,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.01, \n",
    "    height_shift_range=0.01, \n",
    "    horizontal_flip=False, \n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "gen = datagen.flow(X_train[1:2], batch_size=1)\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gen.next().squeeze())\n",
    "    plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                input_shape=(28,28,1)))\n",
    "model_2.add(MaxPooling2D((2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "model_2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "model_2.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model_2.add(Dropout(0.4))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(128, activation='relu'))\n",
    "model_2.add(Dropout(0.3))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(10, activation='softmax'))\n",
    "#loop through the optimizers\n",
    "model_2.compile(optimizer='adam',\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "metrics=['accuracy'])\n",
    "    \n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_accuracy', patience = 5)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "model_checkpoint = ModelCheckpoint('data_aug_best_model2_DO.hdf5', save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_data_augmented = model_2.fit(X_train, y_train,  \n",
    "        batch_size=256,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          callbacks = [monitor_val_acc, model_checkpoint],\n",
    "          validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy:', model_2_data_augmented.model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_2_data_augmented.model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "accuracy = model_2_data_augmented.history['accuracy']\n",
    "val_accuracy = model_2_data_augmented.history['val_accuracy']\n",
    "loss = model_2_data_augmented.history['loss']\n",
    "val_loss = model_2_data_augmented.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'ro', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model_2_data_augmented.model.predict(x_test)\n",
    "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#outputing as classifcation matrix to see the accuracy of each class\n",
    "\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "         \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
    "#get the indices to be plotted\n",
    "correct = np.nonzero(predicted_classes==y_test)[0]\n",
    "incorrect = np.nonzero(predicted_classes!=y_test)[0]\n",
    "target_names = [\"Item {} {}\".format(labels[i], i) for i in range(10)]\n",
    "print(classification_report(y_test, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take notes\n",
    "- function to visualize graphs nicely can copy xavier's code\n",
    "- how to k fold follow mastery\n",
    "- everytime finish should try out on test score\n",
    "- use a constant seed value ( initialization can be normalized refere to prac 3)\n",
    "- have i used to_categorical and why should i?\n",
    "- Batch size training with and without\n",
    "- kernel_initializer in adding dense layer. It is to have normal distribution when randomly choosing the weights\n",
    "- idea behind filtering , training the filters which then multiplies.\n",
    "- max pooling and average pooling ( shrinking down the layers)\n",
    "- Why is Flattening to 1D useful and comparison.\n",
    "- dropout should be 0.2/0.5 to 0.8 in keras layer try dropout only\n",
    "- batch normalization\n",
    "- l2 regularization\n",
    "- data augmentation\n",
    "\n",
    "\n",
    "\n",
    "- general way to tell how many neurons is looking at data points then it should be comparable to the parameters( CNN comes in to make neurons in each layer of CNN smaller )\n",
    "\n",
    "- All optimization tried and explained\n",
    "- weight decay\n",
    "- representation scarsity\n",
    "\n",
    "\n",
    "\n",
    "- functional api merge cnn \n",
    "- LeNet\n",
    "- AlexNet\n",
    "- VGGNet\n",
    "- GoogLeNet\n",
    "- ResNet\n",
    "- ZFNet\n",
    "\n",
    "## things i don't understand \n",
    "- auto denoising and auto encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7be89cafe156eeb38bc0fd9260fac01b40a1b8168704394d7ee38fe03ea6e8a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
